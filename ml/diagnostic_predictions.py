"""
Script de Diagnostic Complet - Analyse PrÃ©dictive
Identifie et rÃ©sout les problÃ¨mes de prÃ©dictions Ã  zÃ©ro

Usage: python diagnostic_predictions.py
"""

import os
import json
import joblib
import numpy as np
import pandas as pd
from pathlib import Path

# Configuration
MODELS_DIR = Path(__file__).parent / 'models'
REQUIRED_FILES = [
    'ranking_model.pkl',
    'bestseller_model.pkl',
    'price_recommendation_model.pkl',
    'feature_scaler.pkl',
    'label_encoders.pkl',
    'training_metrics.json'
]


def print_section(title):
    """Affiche une section."""
    print("\n" + "="*70)
    print(f"  {title}")
    print("="*70)


def check_files_exist():
    """VÃ©rifie que tous les fichiers nÃ©cessaires existent."""
    print_section("ğŸ” VÃ‰RIFICATION DES FICHIERS")

    all_exist = True
    for filename in REQUIRED_FILES:
        filepath = MODELS_DIR / filename
        exists = filepath.exists()
        status = "âœ…" if exists else "âŒ"
        size = filepath.stat().st_size if exists else 0
        print(f"{status} {filename:40} ({size:,} bytes)")
        if not exists:
            all_exist = False

    if not all_exist:
        print("\nâŒ PROBLÃˆME: Certains fichiers sont manquants!")
        print("ğŸ’¡ Solution: ExÃ©cutez 'python train_model.py'")
        return False

    print("\nâœ… Tous les fichiers sont prÃ©sents")
    return True


def load_and_check_models():
    """Charge et vÃ©rifie les modÃ¨les."""
    print_section("ğŸ¤– CHARGEMENT DES MODÃˆLES")

    try:
        # Charger le scaler
        scaler_path = MODELS_DIR / 'feature_scaler.pkl'
        scaler = joblib.load(scaler_path)
        print(f"âœ… Scaler chargÃ©")
        print(f"   Features: {scaler.n_features_in_}")
        print(f"   Mean: {scaler.mean_[:3]}... (premiers 3)")
        print(f"   Std: {scaler.scale_[:3]}... (premiers 3)")

        # VÃ©rifier si le scaler a des valeurs valides
        if np.any(scaler.scale_ == 0):
            print("   âš ï¸ ATTENTION: Certains scale_ sont Ã  zÃ©ro!")
            zero_features = np.where(scaler.scale_ == 0)[0]
            print(f"   Features avec scale=0: {zero_features}")

        # Charger les encoders
        encoders_path = MODELS_DIR / 'label_encoders.pkl'
        encoders = joblib.load(encoders_path)
        print(f"\nâœ… Encoders chargÃ©s")
        for name, encoder in encoders.items():
            print(f"   {name}: {len(encoder.classes_)} classes")
            print(f"      Classes: {encoder.classes_[:5]}...")

        # Charger les modÃ¨les
        models = {}
        for model_name, filename in [
            ('ranking', 'ranking_model.pkl'),
            ('bestseller', 'bestseller_model.pkl'),
            ('price', 'price_recommendation_model.pkl')
        ]:
            model_path = MODELS_DIR / filename
            models[model_name] = joblib.load(model_path)
            print(f"\nâœ… ModÃ¨le {model_name} chargÃ©")
            print(f"   Type: {type(models[model_name]).__name__}")

            if hasattr(models[model_name], 'feature_importances_'):
                print(f"   Features importance (top 3):")
                importances = models[model_name].feature_importances_
                top_3_idx = np.argsort(importances)[-3:][::-1]
                for idx in top_3_idx:
                    print(f"      Feature {idx}: {importances[idx]:.4f}")

        return scaler, encoders, models

    except Exception as e:
        print(f"\nâŒ ERREUR lors du chargement: {e}")
        import traceback
        traceback.print_exc()
        return None, None, None


def check_training_metrics():
    """VÃ©rifie les mÃ©triques d'entraÃ®nement."""
    print_section("ğŸ“Š MÃ‰TRIQUES D'ENTRAÃNEMENT")

    try:
        metrics_path = MODELS_DIR / 'training_metrics.json'
        with open(metrics_path, 'r') as f:
            metrics = json.load(f)

        print("MÃ©tadonnÃ©es:")
        metadata = metrics.get('metadata', {})
        print(f"   Version: {metadata.get('version', 'N/A')}")
        print(f"   EntraÃ®nÃ© le: {metadata.get('trained_at', 'N/A')}")
        print(f"   DonnÃ©es rÃ©elles: {metadata.get('real_data_count', 0)}")
        print(f"   Source: {metadata.get('data_source', 'N/A')}")
        print(f"   Features: {len(metadata.get('feature_names', []))}")

        print("\nPerformances:")
        for model_name in ['ranking', 'bestseller', 'price']:
            if model_name in metrics:
                model_metrics = metrics[model_name]
                print(f"\n   {model_name.upper()}:")

                if model_name == 'ranking':
                    print(f"      RÂ²: {model_metrics.get('r2_score', 0):.4f}")
                    print(f"      RMSE: {model_metrics.get('rmse', 0):.2f}")
                    print(f"      MAE: {model_metrics.get('mae', 0):.2f}")

                elif model_name == 'bestseller':
                    print(f"      F1: {model_metrics.get('f1_score', 0):.4f}")
                    print(f"      Precision: {model_metrics.get('precision', 0):.4f}")
                    print(f"      Recall: {model_metrics.get('recall', 0):.4f}")

                elif model_name == 'price':
                    print(f"      RÂ²: {model_metrics.get('r2_score', 0):.4f}")
                    print(f"      RMSE: ${model_metrics.get('rmse', 0):.2f}")
                    print(f"      MAPE: {model_metrics.get('mape', 0):.2f}%")

                n_samples = model_metrics.get('n_samples', 0)
                print(f"      Samples: {n_samples}")

                if n_samples < 50:
                    print(f"      âš ï¸ ATTENTION: Peu de donnÃ©es d'entraÃ®nement!")

        return metrics

    except Exception as e:
        print(f"âŒ ERREUR: {e}")
        return None


def test_prediction(scaler, encoders, models):
    """Test une prÃ©diction rÃ©elle."""
    print_section("ğŸ§ª TEST DE PRÃ‰DICTION")

    # CrÃ©er un produit test RÃ‰ALISTE
    test_product = {
        'current_price': 29.99,
        'rating': 4.2,
        'review_count': 150,
        'sales_count': 45,
        'stock_quantity': 100,
        'days_since_listed': 60,
        'seller_rating': 4.5,
        'discount_percentage': 10.0,
        'category': 'Electronics'
    }

    print("Produit test:")
    for key, value in test_product.items():
        print(f"   {key}: {value}")

    try:
        # PrÃ©parer les features
        features = [
            test_product['current_price'],
            test_product['rating'],
            test_product['review_count'],
            test_product['sales_count'],
            test_product['stock_quantity'],
            test_product['days_since_listed'],
            test_product['seller_rating'],
            test_product['discount_percentage']
        ]

        # Encoder la catÃ©gorie
        if 'category' in encoders:
            try:
                cat_encoded = encoders['category'].transform([test_product['category']])[0]
            except:
                cat_encoded = 0
                print("   âš ï¸ CatÃ©gorie inconnue, utilisation de 0")
        else:
            cat_encoded = 0

        features.append(float(cat_encoded))

        print(f"\n   Features brutes: {features}")

        # Normaliser
        X = np.array([features])
        print(f"   Shape avant scaling: {X.shape}")

        X_scaled = scaler.transform(X)
        print(f"   Features normalisÃ©es: {X_scaled[0][:3]}... (premiers 3)")

        # VÃ©rifier si les features normalisÃ©es sont valides
        if np.any(np.isnan(X_scaled)):
            print("   âŒ ERREUR: Features normalisÃ©es contiennent des NaN!")
            return False

        if np.all(X_scaled == 0):
            print("   âŒ ERREUR: Toutes les features normalisÃ©es sont Ã  zÃ©ro!")
            return False

        # Test ranking
        print("\n   ğŸ† RANKING:")
        ranking_pred = models['ranking'].predict(X_scaled)[0]
        print(f"      PrÃ©diction: {ranking_pred:.2f}")
        if ranking_pred == 0:
            print("      âŒ PROBLÃˆME: PrÃ©diction Ã  zÃ©ro!")
        else:
            print("      âœ… PrÃ©diction valide")

        # Test bestseller
        print("\n   â­ BESTSELLER:")
        bestseller_prob = models['bestseller'].predict_proba(X_scaled)[0][1]
        print(f"      ProbabilitÃ©: {bestseller_prob:.4f} ({bestseller_prob*100:.2f}%)")
        if bestseller_prob == 0:
            print("      âŒ PROBLÃˆME: ProbabilitÃ© Ã  zÃ©ro!")
        else:
            print("      âœ… PrÃ©diction valide")

        # Test price
        print("\n   ğŸ’° PRIX:")
        price_pred = models['price'].predict(X_scaled)[0]
        print(f"      Prix recommandÃ©: ${price_pred:.2f}")
        print(f"      Prix actuel: ${test_product['current_price']:.2f}")
        print(f"      DiffÃ©rence: ${price_pred - test_product['current_price']:.2f}")
        if price_pred == 0:
            print("      âŒ PROBLÃˆME: Prix Ã  zÃ©ro!")
        else:
            print("      âœ… PrÃ©diction valide")

        # RÃ©sumÃ©
        all_valid = (ranking_pred != 0 and bestseller_prob != 0 and price_pred != 0)
        if all_valid:
            print("\nâœ… TOUS LES MODÃˆLES FONCTIONNENT CORRECTEMENT!")
            return True
        else:
            print("\nâŒ CERTAINS MODÃˆLES RETOURNENT DES ZÃ‰ROS!")
            return False

    except Exception as e:
        print(f"\nâŒ ERREUR lors de la prÃ©diction: {e}")
        import traceback
        traceback.print_exc()
        return False


def analyze_data_distribution():
    """Analyse la distribution des donnÃ©es d'entraÃ®nement."""
    print_section("ğŸ“ˆ ANALYSE DES DONNÃ‰ES D'ENTRAÃNEMENT")

    try:
        metrics_path = MODELS_DIR / 'training_metrics.json'
        with open(metrics_path, 'r') as f:
            metrics = json.load(f)

        metadata = metrics.get('metadata', {})
        n_samples = metadata.get('real_data_count', 0)

        print(f"Nombre de samples: {n_samples}")

        if n_samples == 0:
            print("âŒ PROBLÃˆME CRITIQUE: Aucun Ã©chantillon d'entraÃ®nement!")
            print("\nğŸ’¡ DIAGNOSTIC:")
            print("   Le modÃ¨le a Ã©tÃ© entraÃ®nÃ© sur 0 Ã©chantillons")
            print("   C'est pourquoi toutes les prÃ©dictions sont Ã  zÃ©ro!")
            print("\nğŸ”§ SOLUTION:")
            print("   1. Assurez-vous que la base de donnÃ©es contient des produits")
            print("   2. VÃ©rifiez la connexion au backend Spring Boot")
            print("   3. RÃ©-exÃ©cutez: python train_model.py")
            return False

        elif n_samples < 50:
            print(f"âš ï¸ ATTENTION: TrÃ¨s peu de donnÃ©es ({n_samples} samples)")
            print("   Les prÃ©dictions peuvent Ãªtre imprÃ©cises")
            print("\nğŸ’¡ RECOMMANDATION:")
            print("   Ajoutez plus de produits dans la plateforme (minimum 100)")

        else:
            print(f"âœ… QuantitÃ© de donnÃ©es acceptable ({n_samples} samples)")

        # Analyser les features
        if 'ranking' in metrics:
            feat_imp = metrics['ranking'].get('feature_importance', {})
            if feat_imp:
                print("\nImportance des features (Top 5):")
                sorted_features = sorted(feat_imp.items(), key=lambda x: x[1], reverse=True)[:5]
                for feat, imp in sorted_features:
                    print(f"   {feat:25} {imp:.4f}")

        return True

    except Exception as e:
        print(f"âŒ ERREUR: {e}")
        return False


def provide_recommendations():
    """Fournit des recommandations."""
    print_section("ğŸ’¡ RECOMMANDATIONS")

    print("""
Si vous voyez des zÃ©ros dans le dashboard:

1. ğŸ”„ RÃ‰-ENTRAÃNER LE MODÃˆLE
   â†’ python train_model.py
   â†’ Assurez-vous que des produits existent dans la BDD
   â†’ VÃ©rifiez que le backend est dÃ©marrÃ© (port 8080)

2. âœ… VÃ‰RIFIER LES DONNÃ‰ES
   â†’ Le modÃ¨le doit avoir au moins 50 produits pour fonctionner
   â†’ Chaque produit doit avoir un prix > 0
   â†’ Les catÃ©gories doivent Ãªtre dÃ©finies

3. ğŸ”Œ VÃ‰RIFIER LES SERVICES
   â†’ Backend Spring Boot: http://localhost:8080
   â†’ Service Flask ML: http://localhost:5001
   â†’ Test: curl http://localhost:5001/health

4. ğŸ§ª TESTER MANUELLEMENT
   â†’ Utilisez cet outil: python diagnostic_predictions.py
   â†’ Appelez directement l'API Flask
   â†’ VÃ©rifiez les logs du backend

5. ğŸ“Š RÃ‰GÃ‰NÃ‰RER LES PRÃ‰DICTIONS
   â†’ Dans le dashboard: "GÃ©nÃ©rer les prÃ©dictions"
   â†’ Attendez quelques secondes
   â†’ RafraÃ®chissez la page

Si le problÃ¨me persiste:
   â†’ VÃ©rifiez les logs Python et Java
   â†’ Assurez-vous que Flask utilise les bons modÃ¨les
   â†’ Testez avec des donnÃ©es d'exemple
""")


def main():
    """Fonction principale."""
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘          ğŸ” DIAGNOSTIC COMPLET - ANALYSE PRÃ‰DICTIVE           â•‘
â•‘                                                               â•‘
â•‘  Ce script identifie pourquoi les prÃ©dictions sont Ã  zÃ©ro   â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")

    # 1. VÃ©rifier les fichiers
    if not check_files_exist():
        provide_recommendations()
        return

    # 2. Analyser les mÃ©triques
    metrics = check_training_metrics()
    if not metrics:
        provide_recommendations()
        return

    # 3. Analyser la distribution des donnÃ©es
    if not analyze_data_distribution():
        provide_recommendations()
        return

    # 4. Charger les modÃ¨les
    scaler, encoders, models = load_and_check_models()
    if scaler is None:
        provide_recommendations()
        return

    # 5. Tester une prÃ©diction
    success = test_prediction(scaler, encoders, models)

    # 6. Conclusion
    print_section("ğŸ¯ CONCLUSION")
    if success:
        print("""
âœ… DIAGNOSTIC COMPLET: TOUT FONCTIONNE!

Les modÃ¨les sont correctement entraÃ®nÃ©s et retournent des valeurs.
Si vous voyez toujours des zÃ©ros dans le dashboard:

1. VÃ©rifiez que Flask utilise ces modÃ¨les
   â†’ RedÃ©marrez: python app.py

2. RÃ©gÃ©nÃ©rez les prÃ©dictions
   â†’ Dashboard â†’ "GÃ©nÃ©rer les prÃ©dictions"

3. VÃ©rifiez les logs du backend Spring Boot
   â†’ Les prÃ©dictions sont-elles sauvegardÃ©es en BDD?
""")
    else:
        print("""
âŒ PROBLÃˆME DÃ‰TECTÃ‰!

Les modÃ¨les retournent des zÃ©ros. Causes possibles:

1. ModÃ¨le entraÃ®nÃ© sur 0 Ã©chantillons
   â†’ Solution: python train_model.py (avec donnÃ©es)

2. Features mal normalisÃ©es
   â†’ Le scaler a des problÃ¨mes

3. Mauvais encodage de catÃ©gories
   â†’ VÃ©rifiez label_encoders.pkl
""")
        provide_recommendations()


if __name__ == '__main__':
    main()